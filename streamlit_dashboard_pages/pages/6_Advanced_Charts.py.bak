"""
Advanced Charts page for Thesis Allocation System
"""
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import numpy as np
from pathlib import Path
import sys
import tempfile
import os

# Add parent and project root to path for imports
try:
    current_dir = Path(__file__).parent
    parent_dir = current_dir.parent  # streamlit_dashboard_pages/
    project_root = parent_dir.parent  # project root
    sys.path.insert(0, str(project_root))
    sys.path.insert(0, str(parent_dir))
except NameError:
    # Fallback for testing
    project_root = Path.cwd()
    sys.path.insert(0, str(project_root))
    sys.path.insert(0, str(project_root / "streamlit_dashboard_pages"))

from shared import initialize_session_state, save_visualization, calculate_fairness_score

# Initialize session state
initialize_session_state()

# Set page config
st.set_page_config(
    page_title="Advanced Charts - Thesis Allocation Dashboard",
    page_icon="üìà",
    layout="wide"
)

st.header("üìà Advanced Charts")

st.info("""
üîç **Advanced Visualizations:**
- **Sankey Diagram**: Flow from Students ‚Üí Topics ‚Üí Coaches ‚Üí Departments (colored by preference rank)
- **Network Flow**: Network graph showing allocations as connections
- **Cost Matrix**: Heatmap of costs for student-topic pairs
- **Statistics**: Summary statistics and diagnostics
""")

# Explanation of Preference Rank
with st.expander("üìö What is Preference Rank?", expanded=False):
    st.markdown("""
    ### üéØ **Preference Rank Explained**
    
    **Preference Rank** is a numeric value that indicates how satisfied a student is with their assigned topic.
    
    ### üìä **Preference Rank Values:**
    
    **üü¢ Excellent Satisfaction (Tiers):**
    - **0** = Tier 1 (most preferred topics - indifference group)
    - **1** = Tier 2 (moderately preferred topics - indifference group)  
    - **2** = Tier 3 (least preferred topics - indifference group)
    
    **üü° Good Satisfaction (Ranked Preferences):**
    - **10** = 1st choice (student's #1 ranked topic)
    - **11** = 2nd choice (student's #2 ranked topic)
    - **12** = 3rd choice (student's #3 ranked topic)
    - **13** = 4th choice (student's #4 ranked topic)
    - **14** = 5th choice (student's #5 ranked topic)
    
    **üî¥ Poor Satisfaction:**
    - **999** = Unranked topic (student didn't rank this topic at all)
    
    **‚ö° Special Cases:**
    - **-1** = Forced assignment (student was manually assigned to this topic)
    
    ### üé® **Color Coding in Visualizations:**
    - **Green** = Low preference rank (0-2, 10-11) = Happy students
    - **Yellow** = Medium preference rank (12-13) = Moderately satisfied
    - **Red** = High preference rank (14, 999) = Unhappy students
    
    ### üí° **How to Interpret:**
    - **Lower numbers = Better satisfaction** (0 is best, 999 is worst)
    - **Tiers (0-2) are always considered good** regardless of configuration
    - **Ranked preferences (10-14) depend on student's actual ranking**
    - **Unranked (999) means student didn't care about this topic**
    """)

# Sankey Visualization
st.divider()
st.subheader("üåä Sankey Diagram (Student ‚Üí Topic ‚Üí Coach ‚Üí Department)")
st.write("Shows the flow of allocations with colors representing preference satisfaction (green=good, red=bad).")

# Check if cached data is available
if st.session_state.last_allocation_rows is not None and st.session_state.last_repos is not None:
    st.info("‚úÖ Using cached Sankey data from recent allocation.")
    try:
        rows = st.session_state.last_allocation_rows
        
        # Convert namedtuple rows to dictionaries
        rows_dicts = [
            {
                'student': row.student,
                'assigned_topic': row.assigned_topic,
                'assigned_coach': row.assigned_coach,
                'department_id': row.department_id,
                'preference_rank': str(row.preference_rank),
                'effective_cost': str(row.effective_cost)
            }
            for row in rows
        ]
        
        # Import Sankey module
        from viz_sankey_enhanced import create_sankey_html as create_sankey
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as tmp:
            temp_sankey_path = tmp.name
        
        # Create Sankey HTML
        _ = create_sankey(rows_dicts, temp_sankey_path)
        
        # Read the generated HTML
        with open(temp_sankey_path, 'r') as f:
            sankey_html = f.read()
        
        # Save to visualizations folder
        sankey_path = save_visualization(sankey_html, "sankey_diagram.html")
        st.success(f"‚úì Sankey saved to: {sankey_path}")
        
        # Display as HTML
        st.components.v1.html(sankey_html, height=800, scrolling=True)
        
        # Clean up temp file
        try:
            os.remove(temp_sankey_path)
        except:
            pass
            
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Could not generate Sankey diagram: {str(e)}")
        import traceback
        st.error(traceback.format_exc())
else:
    st.warning("üëÜ Run allocation first to generate Sankey diagram")

st.divider()

# Cost Heatmaps
st.subheader("üî• Cost Matrix Heatmaps")

with st.expander("‚ÑπÔ∏è What is Effective Cost?"):
    st.write("""
    **Effective Cost** is a numerical measure of how well each student-topic assignment matches the student's preferences:
    
    - **Lower cost = Better match** ‚úÖ Student got their preference
    - **Higher cost = Worse match** ‚ùå Student got a less preferred topic
    
    **Cost calculation:**
    - 1st ranked choice: Low cost (student wants this most)
    - 2nd ranked choice: Slightly higher cost
    - 3rd, 4th, 5th ranked choices: Increasing cost
    - Unranked topics: Very high cost (not in student's preferences)
    - Tier preferences: Intermediate costs
    
    **In the heatmap:**
    - üü¢ Green = Low cost (good assignment)
    - üü° Yellow = Medium cost (acceptable)
    - üî¥ Red = High cost (poor assignment)
    
    The allocation algorithm tries to **minimize total cost** across all students.
    """)

# Check if we have cached allocation
if st.session_state.last_allocation is not None:
    st.info("‚úÖ Using cached allocation data")
    df = st.session_state.last_allocation
    
    # Student √ó Topic Heatmap
    st.subheader("üìä Student √ó Topic Heatmap")
    
    if 'effective_cost' in df.columns and 'student' in df.columns and 'assigned_topic' in df.columns:
        cost_pivot = df.pivot_table(
            values='effective_cost',
            index='student',
            columns='assigned_topic',
            fill_value=0,
            aggfunc='first'
        )
        
        fig = go.Figure(data=go.Heatmap(
            z=cost_pivot.values,
            x=cost_pivot.columns,
            y=cost_pivot.index,
            colorscale='RdYlGn_r',
            hovertemplate='Student: %{y}<br>Topic: %{x}<br>Cost: %{z}<extra></extra>'
        ))
        fig.update_layout(
            title=f"Effective Cost Heatmap - All {len(df)} Students",
            height=max(600, len(df) * 10),
            xaxis_title="Topic",
            yaxis_title="Student",
            xaxis_tickangle=-45
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("Required columns not found")
    
    # Coach √ó Topic Heatmap
    st.divider()
    st.subheader("üë• Coach √ó Topic Heatmap")
    
    if 'assigned_coach' in df.columns and 'assigned_topic' in df.columns and 'effective_cost' in df.columns:
        coach_topic_cost = df.groupby(['assigned_coach', 'assigned_topic'])['effective_cost'].agg(['sum', 'count']).reset_index()
        
        cost_pivot_coach = coach_topic_cost.pivot_table(
            values='sum',
            index='assigned_coach',
            columns='assigned_topic',
            fill_value=0
        )
        
        fig = go.Figure(data=go.Heatmap(
            z=cost_pivot_coach.values,
            x=cost_pivot_coach.columns,
            y=cost_pivot_coach.index,
            colorscale='RdYlGn_r',
            hovertemplate='Coach: %{y}<br>Topic: %{x}<br>Total Cost: %{z}<extra></extra>'
        ))
        fig.update_layout(
            title="Coach √ó Topic Cost Distribution",
            height=400,
            xaxis_title="Topic",
            yaxis_title="Coach",
            xaxis_tickangle=-45
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Department √ó Topic Heatmap
    st.divider()
    st.subheader("üèõÔ∏è Department √ó Topic Heatmap")
    
    if 'department_id' in df.columns and 'assigned_topic' in df.columns and 'effective_cost' in df.columns:
        dept_topic_cost = df.groupby(['department_id', 'assigned_topic'])['effective_cost'].agg(['sum', 'count']).reset_index()
        
        cost_pivot_dept = dept_topic_cost.pivot_table(
            values='sum',
            index='department_id',
            columns='assigned_topic',
            fill_value=0
        )
        
        fig = go.Figure(data=go.Heatmap(
            z=cost_pivot_dept.values,
            x=cost_pivot_dept.columns,
            y=cost_pivot_dept.index,
            colorscale='RdYlGn_r',
            hovertemplate='Department: %{y}<br>Topic: %{x}<br>Total Cost: %{z}<extra></extra>'
        ))
        fig.update_layout(
            title="Department √ó Topic Cost Distribution",
            height=400,
            xaxis_title="Topic",
            yaxis_title="Department",
            xaxis_tickangle=-45
        )
        st.plotly_chart(fig, use_container_width=True)
        
else:
    st.warning("üëÜ Run allocation first to generate heatmaps")

st.divider()

# Summary Statistics Section
st.subheader("üìä Summary Statistics")
st.write("Complete allocation results and diagnostics")

if st.session_state.last_summary is not None:
    summary_text = st.session_state.last_summary
    
    # DEBUG: Show raw summary for troubleshooting
    with st.expander("üêõ DEBUG: Show Raw Summary (click to expand)"):
        st.text(summary_text)
    
    # Parse summary data
    lines = summary_text.split('\n')
    
    # Create tabs for different sections
    tab1, tab2, tab3, tab4, tab5 = st.tabs(
        ["üìå Overview", "üéØ Preferences", "üìö Topics", "üë• Coaches", "üèõÔ∏è Departments"]
    )
    
    # TAB 1: OVERVIEW
    with tab1:
        col1, col2, col3, col4 = st.columns(4)
        
        # Extract key metrics
        total_students = len(st.session_state.last_allocation) if st.session_state.last_allocation is not None else 0
        
        # Extract solver status and objective - more robust
        objective_value = "N/A"
        unassignable = 0
        unassigned = 0
        
        for i, line in enumerate(lines):
            line_lower = line.lower()
            
            # Match the actual format from outputs.py: "Objective: 3061.0"
            if line.startswith("Objective:"):
                try:
                    val_str = line.split("Objective:")[-1].strip()
                    # Extract number (could be float or int)
                    import re
                    match = re.search(r'[\d.]+', val_str)
                    if match:
                        objective_value = match.group()
                except Exception as e:
                    pass
            
            # Match: "Unassignable students (no admissible topics): 0"
            if "Unassignable students" in line:
                try:
                    val_str = line.split(":")[-1].strip()
                    unassignable = int(val_str)
                except:
                    pass
            
            # Match: "Unassigned after solve: 0"
            if "Unassigned after solve:" in line:
                try:
                    val_str = line.split(":")[-1].strip()
                    unassigned = int(val_str)
                except:
                    pass
        
        with col1:
            st.metric("Total Students", total_students)
        with col2:
            st.metric("Optimal Cost", objective_value)
        with col3:
            st.metric("Unassignable", unassignable, delta_color="off")
        with col4:
            st.metric("Unassigned", unassigned, delta_color="off")
        
        # Add explanations
        with st.expander("üìñ What Do These Metrics Mean?"):
            col_exp1, col_exp2 = st.columns(2)
            
            with col_exp1:
                st.write("""
                **Total Students**: Number of students in allocation
                
                **Optimal Cost**: Total "cost" of the allocation
                - Lower is better ‚úÖ
                - Measured in arbitrary units
                - Represents how satisfied students are overall
                - Example: Cost 3061 means students are on average fairly satisfied
                - Cost includes preference mismatches + penalties + constraints
                """)
            
            with col_exp2:
                st.write("""
                **Unassignable**: Students with NO possible topics
                - These students cannot be assigned to ANY topic
                - Usually 0 (everyone has some option)
                - If > 0: Check if topic requirements are too strict
                
                **Unassigned**: Students not assigned after solving
                - The solver couldn't find assignments for these students
                - Means constraints were too restrictive
                - If > 0: Try enabling Topic/Coach overflow or relaxing constraints
                """)
        
        # Check for uniqueness
        st.divider()
        if "UNIQUE" in summary_text.upper():
            st.success("‚úì Solution appears UNIQUE (no ties in costs)")
            with st.expander("üìñ What Does Uniqueness Mean?"):
                st.write("""
                **Solution Uniqueness**: Whether this is the ONLY best allocation
                
                ‚úÖ **UNIQUE**: This is the best solution. No other allocation has same cost.
                - Most desirable
                - You can be confident in this allocation
                
                ‚ö†Ô∏è **NOT UNIQUE**: Multiple allocations have equally-good costs
                - The solver picked one arbitrarily
                - Alternative allocations might work equally well
                - Consider running again or tweaking parameters
                """)
        else:
            st.info("Solution uniqueness information not found")
    
    # TAB 2: PREFERENCES
    with tab2:
        st.subheader("üéØ Preference Satisfaction Analysis")
        
        # Parse preference satisfaction data from summary
        pref_data = {}
        lines = summary_text.split('\n')
        
        # Find preference satisfaction section
        pref_start = None
        for i, line in enumerate(lines):
            if "Preference satisfaction:" in line:
                pref_start = i
                break
        
        if pref_start is not None:
            # Parse tier satisfaction
            for i in range(pref_start + 1, min(pref_start + 10, len(lines))):
                line = lines[i].strip()
                if line.startswith("Tier"):
                    if "Tier1:" in line:
                        pref_data["Tier 1"] = int(line.split(":")[1].strip())
                    elif "Tier2:" in line:
                        pref_data["Tier 2"] = int(line.split(":")[1].strip())
                    elif "Tier3:" in line:
                        pref_data["Tier 3"] = int(line.split(":")[1].strip())
                elif line.startswith("Ranked choice satisfaction:"):
                    break
            
            # Parse ranked choice satisfaction
            for i in range(pref_start + 1, min(pref_start + 15, len(lines))):
                line = lines[i].strip()
                if "1st choice:" in line:
                    pref_data["1st Choice"] = int(line.split(":")[1].strip())
                elif "2nd choice:" in line:
                    pref_data["2nd Choice"] = int(line.split(":")[1].strip())
                elif "3rd choice:" in line:
                    pref_data["3rd Choice"] = int(line.split(":")[1].strip())
                elif "4th choice:" in line:
                    pref_data["4th Choice"] = int(line.split(":")[1].strip())
                elif "5th choice:" in line:
                    pref_data["5th Choice"] = int(line.split(":")[1].strip())
                elif "Unranked" in line:
                    pref_data["Unranked"] = int(line.split(":")[1].strip())
        
        if pref_data:
            # Display preference satisfaction metrics
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Tier 1 Satisfaction", pref_data.get("Tier 1", 0))
                st.metric("Tier 2 Satisfaction", pref_data.get("Tier 2", 0))
                st.metric("Tier 3 Satisfaction", pref_data.get("Tier 3", 0))
            
            with col2:
                st.metric("1st Choice", pref_data.get("1st Choice", 0))
                st.metric("2nd Choice", pref_data.get("2nd Choice", 0))
                st.metric("3rd Choice", pref_data.get("3rd Choice", 0))
            
            with col3:
                st.metric("4th Choice", pref_data.get("4th Choice", 0))
                st.metric("5th Choice", pref_data.get("5th Choice", 0))
                st.metric("Unranked", pref_data.get("Unranked", 0))
            
            # Calculate and display satisfaction percentages
            total_students = sum(pref_data.values())
            if total_students > 0:
                st.divider()
                st.subheader("üìä Satisfaction Breakdown")
                
                # Tier satisfaction (0-2)
                tier_total = pref_data.get("Tier 1", 0) + pref_data.get("Tier 2", 0) + pref_data.get("Tier 3", 0)
                tier_percentage = (tier_total / total_students) * 100
                
                # Ranked satisfaction (1st-2nd choices = 10-11)
                # This is "Good Satisfaction" for ranked choices
                ranked_total = pref_data.get("1st Choice", 0) + pref_data.get("2nd Choice", 0)
                ranked_percentage = (ranked_total / total_students) * 100
                
                # Third choice (12) - also acceptable
                third_choice = pref_data.get("3rd Choice", 0)
                third_percentage = (third_choice / total_students) * 100
                
                # Poor satisfaction (4th-5th choices + unranked)
                poor_total = pref_data.get("4th Choice", 0) + pref_data.get("5th Choice", 0) + pref_data.get("Unranked", 0)
                poor_percentage = (poor_total / total_students) * 100
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric(
                        "Excellent Satisfaction (Tiers + 1st-2nd Choice)",
                        f"{(tier_percentage + ranked_percentage):.1f}%",
                        f"{tier_total + ranked_total} students"
                    )
                
                with col2:
                    st.metric(
                        "Acceptable Satisfaction (3rd Choice)",
                        f"{third_percentage:.1f}%",
                        f"{third_choice} students"
                    )
                
                with col3:
                    st.metric(
                        "Poor Satisfaction (4th+ or Unranked)",
                        f"{poor_percentage:.1f}%",
                        f"{poor_total} students"
                    )
                
                # Preference rank explanation
                with st.expander("üìö Preference Rank Values Reference"):
                    st.markdown("""
                    **Preference Rank Values:**
                    - **0-2**: Tiers (excellent satisfaction)
                    - **10-14**: Ranked choices (10=1st, 11=2nd, etc.)
                    - **999**: Unranked (poor satisfaction)
                    - **-1**: Forced assignment
                    
                    **Lower numbers = Better satisfaction**
                    """)
                
                # Satisfaction analysis
                st.divider()
                st.subheader("üéØ Satisfaction Analysis")
                
                if tier_percentage > 50:
                    st.success(f"‚úÖ Excellent! {tier_percentage:.1f}% of students got tier preferences (indifference groups)")
                elif ranked_percentage > 70:
                    st.success(f"‚úÖ Good! {ranked_percentage:.1f}% of students got their top 3 choices")
                elif poor_percentage > 30:
                    st.warning(f"‚ö†Ô∏è Room for improvement: {poor_percentage:.1f}% of students got poor satisfaction (4th+ choice or unranked)")
                else:
                    st.info(f"üìä Balanced allocation: {ranked_percentage:.1f}% good satisfaction, {poor_percentage:.1f}% poor satisfaction")
        
        else:
            st.warning("Preference satisfaction data not found in summary")
            st.info("This data will be available after running an allocation")

    # TAB 3: TOPICS
    with tab3:
        topics_data = {}
        topic_start = None
        
        for i, line in enumerate(lines):
            if "topic utilization:" in line.lower():
                topic_start = i
                break
        
        if topic_start is not None:
            for i in range(topic_start + 1, len(lines)):
                line = lines[i]
                if line.strip() == "" or not line.startswith('  '):
                    break
                if "topic" in line.lower():
                    try:
                        parts = line.split(':')
                        if len(parts) == 2:
                            topic_name = parts[0].strip()
                            util_parts = parts[1].split('/')
                            if len(util_parts) == 2:
                                used = int(util_parts[0].strip())
                                total = int(util_parts[1].strip())
                                pct = (used / total * 100) if total > 0 else 0
                                topics_data[topic_name] = {'used': used, 'total': total, 'pct': pct}
                    except:
                        pass
        
        # Fallback: generate from allocation data if summary parsing failed
        if not topics_data and st.session_state.last_allocation is not None:
            df = st.session_state.last_allocation
            if 'assigned_topic' in df.columns:
                topic_counts = df['assigned_topic'].value_counts()
                for topic_id, count in topic_counts.items():
                    topics_data[topic_id] = {'used': count, 'total': count, 'pct': 100}
        
        # Update capacity from repos if available
        if st.session_state.last_repos:
            repo = st.session_state.last_repos
            for topic_id in topics_data.keys():
                if topic_id in repo.topics:
                    topic = repo.topics[topic_id]
                    topics_data[topic_id]['total'] = topic.topic_cap
                    topics_data[topic_id]['pct'] = (topics_data[topic_id]['used'] / topics_data[topic_id]['total'] * 100) if topics_data[topic_id]['total'] > 0 else 0
        
        if topics_data:
            # Sort alphabetically
            sorted_topics = sorted(topics_data.items())
            topic_names = [t[0] for t in sorted_topics]
            used_counts = [t[1]['used'] for t in sorted_topics]
            total_counts = [t[1]['total'] for t in sorted_topics]
            
            fig = go.Figure(data=[
                go.Bar(name='Used', x=topic_names, y=used_counts, marker_color='#27ae60', opacity=0.8),
                go.Bar(name='Capacity', x=topic_names, y=total_counts, marker_color='#e74c3c', opacity=0.6)
            ])
            fig.update_layout(
                title="Topic Utilization (Green=Used, Red=Capacity)",
                xaxis_title="Topic",
                yaxis_title="Students",
                barmode='group',
                height=400,
                xaxis_tickangle=-45,
                hovermode='x unified'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            st.divider()
            st.write("**Utilization Details:**")
            table_data = []
            for topic, data in topics_data.items():
                table_data.append({
                    'Topic': topic,
                    'Used': data['used'],
                    'Capacity': data['total'],
                    'Utilization %': f"{data['pct']:.1f}%"
                })
            st.dataframe(pd.DataFrame(table_data), use_container_width=True)
        else:
            st.info(f"No topic utilization data found (looked around line {topic_start})")
    
    # TAB 4: COACHES
    with tab4:
        coaches_data = {}
        coach_start = None
        
        for i, line in enumerate(lines):
            if "coach utilization:" in line.lower():
                coach_start = i
                break
        
        if coach_start is not None:
            for i in range(coach_start + 1, len(lines)):
                line = lines[i]
                if line.strip() == "" or not line.startswith('  '):
                    break
                if "coach" in line.lower():
                    try:
                        parts = line.split(':')
                        if len(parts) == 2:
                            coach_name = parts[0].strip()
                            util_parts = parts[1].split('/')
                            if len(util_parts) == 2:
                                used = int(util_parts[0].strip())
                                total = int(util_parts[1].strip())
                                pct = (used / total * 100) if total > 0 else 0
                                coaches_data[coach_name] = {'used': used, 'total': total, 'pct': pct}
                    except:
                        pass
        
        # Fallback: generate from allocation data if summary parsing failed
        if not coaches_data and st.session_state.last_allocation is not None:
            df = st.session_state.last_allocation
            if 'assigned_coach' in df.columns:
                coach_counts = df['assigned_coach'].value_counts()
                for coach_id, count in coach_counts.items():
                    coaches_data[coach_id] = {'used': count, 'total': count, 'pct': 100}
        
        # Update capacity from repos if available
        if st.session_state.last_repos:
            repo = st.session_state.last_repos
            for coach_id in coaches_data.keys():
                if coach_id in repo.coaches:
                    coach = repo.coaches[coach_id]
                    coaches_data[coach_id]['total'] = coach.coach_cap
                    coaches_data[coach_id]['pct'] = (coaches_data[coach_id]['used'] / coaches_data[coach_id]['total'] * 100) if coaches_data[coach_id]['total'] > 0 else 0
        
        if coaches_data:
            # Sort alphabetically
            sorted_coaches = sorted(coaches_data.items())
            coach_names = [c[0] for c in sorted_coaches]
            used_counts = [c[1]['used'] for c in sorted_coaches]
            total_counts = [c[1]['total'] for c in sorted_coaches]
            
            fig = go.Figure(data=[
                go.Bar(name='Used', x=coach_names, y=used_counts, marker_color='#3498db', opacity=0.8),
                go.Bar(name='Capacity', x=coach_names, y=total_counts, marker_color='#e67e22', opacity=0.6)
            ])
            fig.update_layout(
                title="Coach Utilization (Blue=Used, Orange=Capacity)",
                xaxis_title="Coach",
                yaxis_title="Students",
                barmode='group',
                height=400,
                xaxis_tickangle=-45,
                hovermode='x unified'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            st.divider()
            st.write("**Utilization Details:**")
            table_data = []
            for coach, data in coaches_data.items():
                table_data.append({
                    'Coach': coach,
                    'Used': data['used'],
                    'Capacity': data['total'],
                    'Utilization %': f"{data['pct']:.1f}%"
                })
            st.dataframe(pd.DataFrame(table_data), use_container_width=True)
        else:
            st.info(f"No coach utilization data found (looked around line {coach_start})")
    
    # TAB 5: DEPARTMENTS
    with tab5:
        dept_data = {}
        dept_start = None
        
        for i, line in enumerate(lines):
            if "department total" in line.lower():
                dept_start = i
                break
        
        if dept_start is not None:
            for i in range(dept_start + 1, len(lines)):
                line = lines[i]
                if line.strip() == "" or not line.startswith('  '):
                    break
                if "department" in line.lower():
                    try:
                        parts = line.split(':')
                        if len(parts) >= 2:
                            dept_name = parts[0].strip()
                            count_str = parts[1].split('(')[0].strip()
                            count = int(count_str)
                            dept_data[dept_name] = count
                    except:
                        pass
        
        # Fallback: generate from allocation data if summary parsing failed
        if not dept_data and st.session_state.last_allocation is not None:
            df = st.session_state.last_allocation
            if 'department_id' in df.columns:
                dept_counts = df['department_id'].value_counts()
                for dept_id, count in dept_counts.items():
                    dept_data[dept_id] = {'used': count, 'total': count, 'pct': 100}
        
        # Also try to get capacity from repos
        dept_capacity_data = {}
        if st.session_state.last_repos:
            repo = st.session_state.last_repos
            for dept_id, dept in repo.departments.items():
                if dept_id in dept_data:
                    dept_data[dept_id]['total'] = dept.desired_min or dept_data[dept_id]['used']
                    dept_data[dept_id]['pct'] = (dept_data[dept_id]['used'] / dept_data[dept_id]['total'] * 100) if dept_data[dept_id]['total'] > 0 else 0
        
        if dept_data:
            # Convert to dict if it's still using old format
            if isinstance(list(dept_data.values())[0], int):
                dept_data = {k: {'used': v, 'total': v, 'pct': 100} for k, v in dept_data.items()}
            
            # Sort alphabetically
            sorted_depts = sorted(dept_data.items())
            dept_names = [d[0] for d in sorted_depts]
            used_counts = [d[1]['used'] for d in sorted_depts]
            total_counts = [d[1]['total'] for d in sorted_depts]
            
            fig = go.Figure(data=[
                go.Bar(name='Used', x=dept_names, y=used_counts, marker_color='#9b59b6', opacity=0.8),
                go.Bar(name='Capacity', x=dept_names, y=total_counts, marker_color='#e91e63', opacity=0.6)
            ])
            fig.update_layout(
                title="Department Utilization (Purple=Used, Pink=Capacity)",
                xaxis_title="Department",
                yaxis_title="Students",
                barmode='group',
                height=400,
                xaxis_tickangle=-45,
                hovermode='x unified'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            st.divider()
            st.write("**Utilization Details:**")
            table_data = []
            for dept, data in dept_data.items():
                if isinstance(data, dict):
                    table_data.append({
                        'Department': dept,
                        'Used': data['used'],
                        'Capacity': data['total'],
                        'Utilization %': f"{data['pct']:.1f}%"
                    })
                else:
                    table_data.append({
                        'Department': dept,
                        'Used': data,
                        'Capacity': data,
                        'Utilization %': '100.0%'
                    })
            st.dataframe(pd.DataFrame(table_data), use_container_width=True)
        else:
            st.info(f"No department data found (looked around line {dept_start})")
else:
    st.warning("üëÜ Run allocation first to view summary statistics")
# Fairness Metrics Section
st.subheader("‚öñÔ∏è Fairness Metrics")
st.write("Analyze allocation fairness using statistical measures and ethical distribution metrics")

# Comprehensive fairness explanation
with st.expander("üìñ Understanding Fairness Metrics (Click to Learn)"):
    tab_explain1, tab_explain2, tab_explain3 = st.tabs(
        ["üéØ What is Fairness?", "üìä Gini Coefficient", "‚öñÔ∏è Fairness Score"]
    )
    
    with tab_explain1:
        st.write("""
        **Fairness in Allocation** means:
        - Every student gets considered fairly
        - No group is systematically disadvantaged
        - Satisfaction is distributed equitably
        - Load on coaches/topics is reasonable
        
        **Why It Matters:**
        - ‚úÖ Ethical responsibility to treat all students equally
        - ‚úÖ Reduces conflicts and complaints
        - ‚úÖ Ensures legitimate allocations
        - ‚úÖ Builds trust in the system
        
        **What We Measure:**
        1. **Cost Fairness**: Are some students getting much worse assignments than others?
        2. **Preference Satisfaction**: What % of students got choices they wanted?
        3. **Load Balance**: Are coaches/topics distributed fairly?
        4. **Overall Score**: Combined measure of all fairness dimensions
        """)
    
    with tab_explain2:
        st.write("""
        **Gini Coefficient** is a standard measure of inequality (from economics)
        
        **Scale: 0 to 1**
        - **0** = Perfect equality (everyone treated exactly the same)
        - **0.3** = Very fair (minor differences)
        - **0.5** = Moderately fair (some inequality)
        - **0.7** = Unfair (large differences)
        - **1** = Perfect inequality (one person has everything)
        
        **In Allocation Context:**
        
        **Example 1 - Fair Allocation** üü¢
        - All students get costs: 10, 12, 11, 13
        - Gini = 0.08 (very fair!)
        - Everyone is equally happy
        
        **Example 2 - Unfair Allocation** üî¥
        - Students get costs: 10, 10, 10, 1000
        - Gini = 0.62 (very unfair!)
        - Most are happy but one is very upset
        
        **Formula:**
        Gini = (2 * Œ£(i * x_i)) / (n * Œ£x_i) - (n + 1) / n
        
        Where x_i are values in sorted order.
        """)
    
    with tab_explain3:
        st.write("""
        **Fairness Score** combines multiple metrics into 0-100 scale
        
        **Components & Weights:**
        - **40%** - Preference Satisfaction (most important!)
          - How many students got their ranked choices?
        - **20%** - Cost Fairness (using Gini coefficient)
          - Are some students treated much worse?
        - **15%** - Topic Load Balance
          - Are topics equally filled?
        - **15%** - Coach Load Balance
          - Are coaches equally loaded?
        - **10%** - Department Load Balance
          - Are departments equally filled?
        
        **Score Interpretation:**
        - **80-100 ‚úÖ**: Excellent - Allocation is ethically sound
          - Students are satisfied and treated fairly
          - Recommend proceeding with this allocation
          
        - **60-79 ‚ö†Ô∏è**: Good - Improvements possible
          - Acceptable but some issues identified
          - Consider tweaking parameters
          
        - **Below 60 ‚ùå**: Poor - Needs review
          - Significant fairness issues detected
          - Suggest relaxing constraints or adjusting weights
        
        **Target**: Aim for 75+ for balanced allocations
        """)

# Check if we have cached allocation
if st.session_state.last_allocation is not None:
    st.info("‚úÖ Using cached allocation data")
    df = st.session_state.last_allocation
    
    try:
        # Calculate fairness metrics
        metrics = calculate_fairness_score(df)
        
        # Display main fairness metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            gini_cost = metrics.get('gini_cost', 0)
            st.metric(
                "Cost Fairness (Gini)",
                f"{gini_cost:.3f}",
                delta="‚Üê Lower is fairer" if gini_cost < 0.5 else "‚Üê Higher = more unequal",
                delta_color="inverse"
            )
            st.caption("0=Perfect equality, 1=Complete inequality")
        
        with col2:
            topic_balance = metrics.get('topic_balance', 0)
            st.metric(
                "Topic Load Balance",
                f"{topic_balance:.1%}",
                delta="‚Üê Higher is more balanced" if topic_balance > 0.5 else "‚Üê Topics imbalanced",
                delta_color="normal"
            )
            st.caption("How evenly topics are filled")
        
        with col3:
            coach_balance = metrics.get('coach_balance', 0)
            st.metric(
                "Coach Load Balance",
                f"{coach_balance:.1%}",
                delta="‚Üê Higher is more balanced",
                delta_color="normal"
            )
            st.caption("How evenly coaches are assigned")
        
        with col4:
            ranked_sat = metrics.get('ranked_satisfaction', 0)
            st.metric(
                "Preference Satisfaction",
                f"{ranked_sat:.1%}",
                delta="‚Üê Students getting ranked choices",
                delta_color="normal"
            )
            st.caption("Students satisfied with assignment")
        
        st.divider()
        
        # Detailed fairness analysis
        tab_f1, tab_f2, tab_f3, tab_f4 = st.tabs(
            ["üìä Cost Distribution", "üéØ Preference Fairness", "‚öñÔ∏è Load Balance", "üìä Normalized Load Balance"]
        )
        
        # TAB 1: Cost Distribution
        with tab_f1:
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Cost Statistics:**")
                cost_stats = {
                    'Mean Cost': f"{metrics.get('cost_mean', 0):.2f}",
                    'Median Cost': f"{metrics.get('cost_median', 0):.2f}",
                    'Std Dev': f"{metrics.get('cost_std', 0):.2f}",
                    'Coeff of Variation': f"{metrics.get('cost_cv', 0):.3f}",
                    'Gini Coefficient': f"{metrics.get('gini_cost', 0):.3f}",
                }
                stats_df = pd.DataFrame(list(cost_stats.items()), columns=['Metric', 'Value'])
                st.dataframe(stats_df, use_container_width=True)
                
                st.write("""
                **Interpretation:**
                - **Gini < 0.3**: Very fair distribution of costs
                - **Gini 0.3-0.5**: Moderately fair
                - **Gini > 0.5**: Unequal distribution (some students get bad matches)
                
                **Coefficient of Variation:**
                - **< 0.5**: Low variability (consistent fairness)
                - **> 1.0**: High variability (unfair allocation)
                """)
            
            with col2:
                # Cost distribution histogram
                costs = df['effective_cost'].values
                fig = go.Figure()
                fig.add_trace(go.Histogram(
                    x=costs,
                    nbinsx=30,
                    marker_color='#3498db',
                    name='Students',
                    opacity=0.7
                ))
                fig.add_vline(
                    x=np.mean(costs),
                    line_dash="dash",
                    line_color="red",
                    name=f"Mean: {np.mean(costs):.2f}"
                )
                fig.add_vline(
                    x=np.median(costs),
                    line_dash="dash",
                    line_color="green",
                    name=f"Median: {np.median(costs):.2f}"
                )
                fig.update_layout(
                    title="Cost Distribution Across Students",
                    xaxis_title="Effective Cost",
                    yaxis_title="Number of Students",
                    height=400,
                    showlegend=True
                )
                st.plotly_chart(fig, use_container_width=True)
        
        # TAB 2: Preference Fairness
        with tab_f2:
            col1, col2 = st.columns(2)
            
            with col1:
                # Preference rank distribution
                pref_dist = df['preference_rank'].value_counts().sort_index()
                
                # Categorize preferences
                ranked_students = len(df[df['preference_rank'].between(10, 14)])
                tier_students = len(df[df['preference_rank'].between(0, 2)])
                unranked_students = len(df[df['preference_rank'] == 999])
                forced_students = len(df[df['preference_rank'] == -1])
                other_students = len(df) - ranked_students - tier_students - unranked_students - forced_students
                
                pref_categories = {
                    'üéØ Got Ranked Choice': ranked_students,
                    '‚≠ê Got Tier Preference': tier_students,
                    '‚ùå Got Unranked': unranked_students,
                    'üîß Forced Assignment': forced_students,
                    '‚ùì Other': other_students
                }
                
                fig = px.pie(
                    values=list(pref_categories.values()),
                    names=list(pref_categories.keys()),
                    title="Student Satisfaction Distribution",
                    color_discrete_sequence=['#27ae60', '#3498db', '#e74c3c', '#f39c12', '#95a5a6']
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.write("**Preference Satisfaction Summary:**")
                total = len(df)
                pref_summary = {
                    'Metric': [
                        '‚úÖ Got Ranked Choice (1st-5th)',
                        '‚≠ê Got Tier Preference',
                        '‚ùå Got Unranked Topic',
                        'üîß Forced Assignment',
                        'üìä Total Students'
                    ],
                    'Count': [
                        ranked_students,
                        tier_students,
                        unranked_students,
                        forced_students,
                        total
                    ],
                    'Percentage': [
                        f"{ranked_students/total*100:.1f}%" if total > 0 else "0%",
                        f"{tier_students/total*100:.1f}%" if total > 0 else "0%",
                        f"{unranked_students/total*100:.1f}%" if total > 0 else "0%",
                        f"{forced_students/total*100:.1f}%" if total > 0 else "0%",
                        "100%"
                    ]
                }
                pref_df = pd.DataFrame(pref_summary)
                st.dataframe(pref_df, use_container_width=True)
                
                st.write("""
                **Fairness Interpretation:**
                - **Got Ranked Choice > 70%**: Very fair ‚úÖ
                - **Got Ranked Choice 50-70%**: Acceptable ‚ö†Ô∏è
                - **Got Ranked Choice < 50%**: Unfair ‚ùå
                """)
        
        # TAB 3: Load Balance
        with tab_f3:
            # Topic load balance
            if 'assigned_topic' in df.columns:
                topic_counts = df['assigned_topic'].value_counts()
                
                # Get all topics from repo and sort alphabetically
                if st.session_state.last_repos:
                    repo = st.session_state.last_repos
                    all_topics = sorted(repo.topics.keys())
                    # Create counts for all topics, filling with 0 for unused ones
                    topic_counts_full = {t: topic_counts.get(t, 0) for t in all_topics}
                else:
                    # Fallback: sort by index
                    topic_counts_full = dict(sorted(topic_counts.items()))
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    x=list(topic_counts_full.keys()),
                    y=list(topic_counts_full.values()),
                    marker_color='#3498db',
                    name='Students'
                ))
                avg_val = sum(topic_counts_full.values()) / len(topic_counts_full) if topic_counts_full else 0
                fig.add_hline(
                    y=avg_val,
                    line_dash="dash",
                    line_color="red",
                    annotation_text=f"Avg: {avg_val:.1f}"
                )
                fig.update_layout(
                    title=f"Topic Load Balance (Gini: {metrics.get('gini_topics', 0):.3f})",
                    xaxis_title="Topic",
                    yaxis_title="Students Assigned",
                    height=400,
                    xaxis_tickangle=-45
                )
                st.plotly_chart(fig, use_container_width=True)
            
            st.divider()
            
            # Coach load balance
            if 'assigned_coach' in df.columns:
                coach_counts = df['assigned_coach'].value_counts()
                
                # Get all coaches from repo and sort alphabetically
                if st.session_state.last_repos:
                    repo = st.session_state.last_repos
                    all_coaches = sorted(repo.coaches.keys())
                    # Create counts for all coaches, filling with 0 for unused ones
                    coach_counts_full = {c: coach_counts.get(c, 0) for c in all_coaches}
                else:
                    # Fallback: sort by index
                    coach_counts_full = dict(sorted(coach_counts.items()))
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    x=list(coach_counts_full.keys()),
                    y=list(coach_counts_full.values()),
                    marker_color='#e67e22',
                    name='Students'
                ))
                avg_val = sum(coach_counts_full.values()) / len(coach_counts_full) if coach_counts_full else 0
                fig.add_hline(
                    y=avg_val,
                    line_dash="dash",
                    line_color="red",
                    annotation_text=f"Avg: {avg_val:.1f}"
                )
                fig.update_layout(
                    title=f"Coach Load Balance (Gini: {metrics.get('gini_coaches', 0):.3f})",
                    xaxis_title="Coach",
                    yaxis_title="Students Assigned",
                    height=400,
                    xaxis_tickangle=-45
                )
                st.plotly_chart(fig, use_container_width=True)
            
            st.divider()
            
            # Department load balance
            if 'department_id' in df.columns:
                dept_counts = df['department_id'].value_counts()
                
                # Get all departments from repo and sort alphabetically
                if st.session_state.last_repos:
                    repo = st.session_state.last_repos
                    all_depts = sorted(repo.departments.keys())
                    # Create counts for all departments, filling with 0 for unused ones
                    dept_counts_full = {d: dept_counts.get(d, 0) for d in all_depts}
                else:
                    # Fallback: sort by index
                    dept_counts_full = dict(sorted(dept_counts.items()))
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    x=list(dept_counts_full.keys()),
                    y=list(dept_counts_full.values()),
                    marker_color='#9b59b6',
                    name='Students'
                ))
                avg_val = sum(dept_counts_full.values()) / len(dept_counts_full) if dept_counts_full else 0
                fig.add_hline(
                    y=avg_val,
                    line_dash="dash",
                    line_color="red",
                    annotation_text=f"Avg: {avg_val:.1f}"
                )
                fig.update_layout(
                    title=f"Department Load Balance (Gini: {metrics.get('gini_depts', 0):.3f})",
                    xaxis_title="Department",
                    yaxis_title="Students Assigned",
                    height=400,
                    xaxis_tickangle=-45
                )
                st.plotly_chart(fig, use_container_width=True)
            
            st.divider()
            st.write("**Load Balance Interpretation:**")
            balance_col1, balance_col2, balance_col3 = st.columns(3)
            
            with balance_col1:
                st.write(f"""
                **Topic Balance: {metrics.get('topic_balance', 0):.1%}**
                
                Shows if topics are equally loaded.
                
                - **> 80%**: Topics well balanced ‚úÖ
                - **60-80%**: Acceptable ‚ö†Ô∏è
                - **< 60%**: Imbalanced ‚ùå
                """)
            
            with balance_col2:
                st.write(f"""
                **Coach Balance: {metrics.get('coach_balance', 0):.1%}**
                
                Shows if coaches are equally loaded.
                
                - **> 80%**: Coaches well balanced ‚úÖ
                - **60-80%**: Acceptable ‚ö†Ô∏è
                - **< 60%**: Imbalanced ‚ùå
                """)
            
            with balance_col3:
                st.write(f"""
                **Dept Balance: {metrics.get('dept_balance', 0):.1%}**
                
                Shows if departments are equally filled.
                
                - **> 80%**: Depts well balanced ‚úÖ
                - **60-80%**: Acceptable ‚ö†Ô∏è
                - **< 60%**: Imbalanced ‚ùå
                """)
        
        # TAB 4: Normalized Load Balance
        with tab_f4:
            st.write("**Normalized Load Balance** - Shows assignments as % of capacity")
            
            # Topic normalized balance (% of capacity)
            if 'assigned_topic' in df.columns:
                topic_counts = df['assigned_topic'].value_counts()
                
                # Get topic capacities from repo (stored in session state)
                try:
                    if st.session_state.last_repos:
                        repo = st.session_state.last_repos
                        topic_data = []
                        for topic_id in sorted(repo.topics.keys()):
                            topic = repo.topics[topic_id]
                            assigned = topic_counts.get(topic_id, 0)
                            capacity = topic.topic_cap
                            normalized = (assigned / capacity * 100) if capacity > 0 else 0
                            topic_data.append({
                                'Topic': topic_id,
                                'Assigned': assigned,
                                'Capacity': capacity,
                                'Usage %': normalized
                            })
                        
                        topic_norm_df = pd.DataFrame(topic_data)
                        
                        fig = px.bar(
                            topic_norm_df,
                            x='Topic',
                            y='Usage %',
                            title='Topic Utilization (% of Capacity)',
                            color='Usage %',
                            color_continuous_scale='RdYlGn_r',
                            labels={'Usage %': 'Capacity Used (%)'},
                            hover_data={'Assigned': True, 'Capacity': True}
                        )
                        fig.add_hline(y=100, line_dash="dash", line_color="red", annotation_text="100% Full")
                        fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="80% Utilization")
                        fig.update_layout(height=400, xaxis_tickangle=-45)
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("Repository data not available")
                except Exception as e:
                    st.warning(f"Could not load topic capacities: {e}")
            
            st.divider()
            
            # Coach normalized balance (% of capacity)
            if 'assigned_coach' in df.columns:
                coach_counts = df['assigned_coach'].value_counts()
                
                try:
                    if st.session_state.last_repos:
                        repo = st.session_state.last_repos
                        coach_data = []
                        for coach_id in sorted(repo.coaches.keys()):
                            coach = repo.coaches[coach_id]
                            assigned = coach_counts.get(coach_id, 0)
                            capacity = coach.coach_cap
                            normalized = (assigned / capacity * 100) if capacity > 0 else 0
                            coach_data.append({
                                'Coach': coach_id,
                                'Assigned': assigned,
                                'Capacity': capacity,
                                'Usage %': normalized
                            })
                        
                        coach_norm_df = pd.DataFrame(coach_data)
                        
                        fig = px.bar(
                            coach_norm_df,
                            x='Coach',
                            y='Usage %',
                            title='Coach Utilization (% of Capacity)',
                            color='Usage %',
                            color_continuous_scale='RdYlGn_r',
                            labels={'Usage %': 'Capacity Used (%)'},
                            hover_data={'Assigned': True, 'Capacity': True}
                        )
                        fig.add_hline(y=100, line_dash="dash", line_color="red", annotation_text="100% Full")
                        fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="80% Utilization")
                        fig.update_layout(height=400, xaxis_tickangle=-45)
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("Repository data not available")
                except Exception as e:
                    st.warning(f"Could not load coach capacities: {e}")
            
            st.divider()
            
            # Department normalized balance (% of capacity)
            if 'department_id' in df.columns:
                dept_counts = df['department_id'].value_counts()
                
                try:
                    if st.session_state.last_repos:
                        repo = st.session_state.last_repos
                        dept_data = []
                        for dept_id in sorted(repo.departments.keys()):
                            dept = repo.departments[dept_id]
                            assigned = dept_counts.get(dept_id, 0)
                            # Use desired_min as capacity if available
                            capacity = dept.desired_min if dept.desired_min > 0 else assigned or 1
                            normalized = (assigned / capacity * 100) if capacity > 0 else 0
                            dept_data.append({
                                'Department': dept_id,
                                'Assigned': assigned,
                                'Capacity': capacity,
                                'Usage %': normalized
                            })
                        
                        dept_norm_df = pd.DataFrame(dept_data)
                        
                        fig = px.bar(
                            dept_norm_df,
                            x='Department',
                            y='Usage %',
                            title='Department Utilization (% of Capacity)',
                            color='Usage %',
                            color_continuous_scale='RdYlGn_r',
                            labels={'Usage %': 'Capacity Used (%)'},
                            hover_data={'Assigned': True, 'Capacity': True}
                        )
                        fig.add_hline(y=100, line_dash="dash", line_color="red", annotation_text="100% Full")
                        fig.add_hline(y=80, line_dash="dash", line_color="orange", annotation_text="80% Utilization")
                        fig.update_layout(height=400, xaxis_tickangle=-45)
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("Repository data not available")
                except Exception as e:
                    st.warning(f"Could not load department data: {e}")
            
            st.divider()
            st.write("""
            **Normalized Load Balance Interpretation:**
            
            This view shows how much each topic/coach is utilized **relative to its capacity**.
            
            - **100%+**: Topic/Coach is overbooked (if overflow enabled)
            - **80-100%**: Well utilized ‚úÖ
            - **50-80%**: Adequate utilization ‚ö†Ô∏è
            - **< 50%**: Underutilized capacity
            """)
        
        # Overall Fairness Score
        st.divider()
        st.subheader("üìà Overall Fairness Score")
        
        # Calculate composite fairness score
        scores = []
        
        # Cost fairness (inverse Gini, 0-100)
        cost_fairness_score = (1 - metrics.get('gini_cost', 0)) * 100
        scores.append(('Cost Fairness', cost_fairness_score))
        
        # Preference satisfaction (already percentage)
        pref_fairness_score = metrics.get('ranked_satisfaction', 0) * 100
        scores.append(('Preference Satisfaction', pref_fairness_score))
        
        # Topic balance (already percentage)
        topic_score = metrics.get('topic_balance', 0) * 100
        scores.append(('Topic Load Balance', topic_score))
        
        # Coach balance
        coach_score = metrics.get('coach_balance', 0) * 100
        scores.append(('Coach Load Balance', coach_score))
        
        # Dept balance
        dept_score = metrics.get('dept_balance', 0) * 100
        scores.append(('Dept Load Balance', dept_score))
        
        # Overall score (weighted average)
        weights = [0.15, 0.3, 0.15, 0.15, 0.1, 0.15]  # More weight on preferences and costs
        overall_score = sum(s * w for (_, s), w in zip(scores, weights))
        
        # Display scores
        col1, col2 = st.columns([2, 1])
        
        with col1:
            score_data = {
                'Fairness Dimension': [s[0] for s in scores] + ['üèÜ OVERALL SCORE'],
                'Score': [f"{s[1]:.1f}/100" for s in scores] + [f"{overall_score:.1f}/100"],
                'Status': [
                    '‚úÖ' if s[1] >= 80 else '‚ö†Ô∏è' if s[1] >= 60 else '‚ùå'
                    for s in scores
                ] + ['‚úÖ' if overall_score >= 75 else '‚ö†Ô∏è' if overall_score >= 60 else '‚ùå']
            }
            score_df = pd.DataFrame(score_data)
            st.dataframe(score_df, use_container_width=True)
        
        with col2:
            # Gauge chart for overall score
            fig = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=overall_score,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "Overall Fairness"},
                delta={'reference': 75, 'prefix': 'vs Target'},
                gauge={
                    'axis': {'range': [0, 100]},
                    'bar': {'color': "darkblue"},
                    'steps': [
                        {'range': [0, 60], 'color': "#f8d7da"},
                        {'range': [60, 80], 'color': "#fff3cd"},
                        {'range': [80, 100], 'color': "#d4edda"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 75
                    }
                }
            ))
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        st.write("""
        **Fairness Score Interpretation:**
        - **80-100**: Excellent fairness ‚úÖ - Allocation is ethically sound
        - **60-79**: Good fairness ‚ö†Ô∏è - Some improvements possible
        - **Below 60**: Poor fairness ‚ùå - Consider revising allocation parameters
        """)
        
        # Add solutions for improving fairness
        st.divider()
        st.subheader("üí° How to Improve Fairness")
        
        gini_cost = metrics.get('gini_cost', 0)
        
        if gini_cost > 0.5:
            st.error(f"‚ö†Ô∏è Cost fairness is low (Gini = {gini_cost:.3f}). High inequality detected!")
            
            with st.expander("üîß Solutions to Improve Cost Fairness"):
                st.write("""
                **Your allocation has high cost inequality.** Here are evidence-based solutions:
                
                ### 1Ô∏è‚É£ **Increase Topic Capacity (Most Impactful)**
                - **Problem**: Popular topics are full, forcing some students to unranked alternatives
                - **Solution**: Add more slots to high-demand topics
                - **Expected Impact**: ‚¨áÔ∏è Gini by 20-40%, major fairness improvement
                - **Action**: Review which topics are over-subscribed and increase their capacity
                
                ### 2Ô∏è‚É£ **Relax Preference Satisfaction Constraints**
                - **Problem**: Min/Max preference constraints may force unfair assignments
                - **Solution**: Remove or loosen constraints to give solver more flexibility
                - **Expected Impact**: ‚¨áÔ∏è Gini by 10-20%
                - **Action**: Go to ‚öôÔ∏è Configuration in the sidebar, set Min Preference to "None" and try again
                
                ### 3Ô∏è‚É£ **Encourage Diverse Student Rankings**
                - **Problem**: Many students rank only 1-2 topics, creating bottlenecks
                - **Solution**: Ask students to rank at least 5-10 different topics
                - **Expected Impact**: ‚¨áÔ∏è Gini by 15-25%
                - **Action**: Communicate to students why diverse rankings help everyone
                
                ### 4Ô∏è‚É£ **Adjust Unranked Cost Parameter**
                - **Problem**: Current unranked cost (100) creates huge jumps in fairness
                - **Solution**: Reduce unranked cost (e.g., from 100 to 50-75)
                - **Expected Impact**: ‚¨áÔ∏è Gini by 5-15%
                - **Action**: Go to ‚öôÔ∏è Configuration in the sidebar, change "Unranked Topic Cost" to lower value
                - **Trade-off**: Students might get fewer preferred topics but fairness improves
                
                ### 5Ô∏è‚É£ **Increase Coach/Topic Load Penalties**
                - **Problem**: Load isn't being balanced fairly
                - **Solution**: Go to ‚öôÔ∏è Configuration in the sidebar, increase P_topic and P_coach penalties
                - **Expected Impact**: ‚¨áÔ∏è Gini by 5-10%
                - **Action**: Try P_topic=1000 and P_coach=800 instead of current values
                
                ### 6Ô∏è‚É£ **Use Tier Preferences Instead of Rankings**
                - **Problem**: Ranked preferences are fine-grained but inflexible
                - **Solution**: Group topics into tiers (must-have, prefer, acceptable)
                - **Expected Impact**: ‚¨áÔ∏è Gini by 10-20%
                - **Action**: Modify student input to use tier system
                
                ### Priority Order for Maximum Impact:
                1. **Increase capacity** (biggest impact, but requires resources)
                2. **Diversify student rankings** (high impact, no resources needed)
                3. **Adjust cost parameters** (medium impact, easy to configure)
                4. **Relax constraints** (medium impact, may reduce other fairness aspects)
                """)
        
        elif gini_cost > 0.3:
            st.warning(f"‚ö†Ô∏è Cost fairness is moderate (Gini = {gini_cost:.3f}). Room for improvement.")
            
            with st.expander("üîß Suggestions to Improve Cost Fairness"):
                st.write(f"""
                **Current Fairness Level**: Good but can be improved
                
                **Quick Wins:**
                - Reduce unranked cost (‚öôÔ∏è Configuration page: Unranked Topic Cost: try 50-75)
                - Ask a few more students to rank additional topics
                - Increase topic overflow penalty slightly (P_topic: +100-200)
                
                **Target**: Get Gini to < 0.2 for excellent fairness
                """)
        
        else:
            st.success(f"‚úÖ Excellent cost fairness (Gini = {gini_cost:.3f})!")
            st.write("Your allocation has fair cost distribution across students. Well done!")
        
        # Additional recommendations based on other metrics
        st.divider()
        st.subheader("üìã Overall Recommendations")
        
        issues = []
        
        if metrics.get('gini_cost', 0) > 0.3:
            issues.append("‚ùå Cost fairness could be improved")
        if metrics.get('topic_balance', 0) < 0.7:
            issues.append("‚ùå Topic load is imbalanced")
        if metrics.get('coach_balance', 0) < 0.7:
            issues.append("‚ùå Coach load is imbalanced")
        if metrics.get('ranked_satisfaction', 0) < 0.5:
            issues.append("‚ùå Less than 50% of students got ranked choices")
        
        if issues:
            st.write("**Issues detected:**")
            for issue in issues:
                st.write(issue)
            
            st.write("""
            **Suggested Next Steps:**
            1. Review the specific metrics in tabs above to understand root causes
            2. Try the recommended solutions in priority order
            3. Re-run allocation with adjusted parameters
            4. Compare fairness before/after by noting the scores
            """)
        else:
            st.success("‚úÖ All fairness metrics are excellent! This is a high-quality allocation.")
        
        st.divider()
    except Exception as e:
        st.error(f"‚ùå Error calculating fairness metrics: {str(e)}")
        import traceback
        st.error(traceback.format_exc())
else:
    st.warning("üëÜ Run allocation first to view fairness metrics")

